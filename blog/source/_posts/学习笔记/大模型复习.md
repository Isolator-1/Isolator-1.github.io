---
title: 大模型复习
tags: [Others]
date: 2025-08-27 11:40:00
categories: [学习笔记]
excerpt: 为了秋招面试😰
---

```
抄自 https://github.com/wdndev/llm_interview_note
```

# LLM 概念

### prefix LLM 和 causal LLM

Prefix LLM 中，Encoder和Decoder共享同一个transformer结构，在生成每个词的时候都可以考虑上下文信息，会根据给定的前缀预测下一个词。

Causal LLM 因果语言模型是一种自回归模型，只有decoder，只能根据前文生成后文，即next token prediction，预测下一个词的概率

### 为什么大部分都是decoder only结构

encoder的双向注意力会存在低秩问题，可能削弱模型表达能力，对于生成任务，也不需要前文看后文的能力，没有好处

decoder-only 在没有任何tuning的数据的情况下，zero shot表现最好，encoder-decoder需要在一定量的标注数据上做multitask finetuning才能激发最佳性能

BERT：encoder only

GPT： decoder only

T5： encoder-decoder

早期的transformer一开始是用来做seq2seq2的，所以才长成encoder+decoder的


### 不同类LLM都适合用在什么场合

bert 适用于通用nlp类任务，例如文本分类，语义相似度计算，主要用在不依赖于特定领域知识或语言风格，但用来做对话和问答效果会变差

LLaMa 是一个decoder only的模型，训练语料主要以英文为主，中日韩文效果会变差

ChatGLM 多用于构建聊天机器人，适合多轮对话处理上下文，并且中英训练语料库1：1

### Word2Vec

早期直接用one-hot编码每个token，缺少token之间的自然相似性， 所以google提出了word2vec，但是被google后来提出的bert几乎完全超越

### BERT 

BERT（Bidirectional Encoder Representations from Transformers）是谷歌提出 ， 大幅提升了NLP领域各个任务的精度，产生了质的飞跃

主要贡献：1. 提出了双向transformer框架  2. 使用了Mask Language Model 和 Next Sentence Prediction 的多任务训练目标

Mask LM ：训练时，有15%的单词被替换，其中有80%会替换为[MASK]，10%换成任意token，10%保留原始Token

Next Sentence Prediction ： BERT接收成对的句子作为苏杭如，并且预测第二个句子是否在原始文档中也是后续句子，训练时50%是对应的前后关系，50%是随机断开的两个句子。


### LlaMA 

LlaMA 与标准 transformers 的不同地方在于 

1. 使用了 前置归一化（归一化放在多头注意力、全连接前面）， 并且 使用RMSNorm而非layernorm； 

2. 使用SwiGLU激活函数

```
Qwen也用了Pre-Norm & RMSNorm。在Transformer模型中，预归一化是最常用的方法，它已被证明比后归一化更能提高训练稳定性。此外，Qwen用RMSNorm取代了传统层归一化技术。这一改变带来了相同的表现水平，同时也提高了效率。

```

# 大模型结构

### MoE 

MoE 包含：一些专家，每个专家都是一个简单的前馈神经网络。
一个可训练的门控网络，它会挑选专家的一个稀疏组合，用来处理每个输入。
所有网络都是使用反向传播联合训练的。

2023年6月，美国知名骇客George Hotz在接受采访时透露，GPT-4由8个220B模型组成；2022年，Google 提出了MoE大模型Switch Transformer，模型大小是1571B，Switch Transformer在预训练任务上显示出比 T5-XXL（11B） 模型更高的样本效率；还有DeepSeek MoE

和其他 MoE 模型的一个显著不同就是，Switch Transformer 的门控网络每次只路由到 1 个 expert，也就是每次只选取 top1 的专家，而其他的模型都是至少 2 个。这样就是最稀疏的 MoE 了，因此单单从 MoE layer 的计算效率上讲是最高的了。

### Attention

Seq2Seq模型是一种基于编码器-解码器结构的模型，主要用于处理序列到序列的任务，例如机器翻译、语音识别等。

传统的Seq2Seq模型只使用编码器来捕捉输入序列的信息，而解码器只从编码器的最后状态中获取信息，并将其用于生成输出序列。

而Attention机制则允许解码器在生成每个输出时，根据输入序列的不同部分给予不同的注意力，从而使得模型更好地关注到输入序列中的重要信息。

### Self Attention 和 Target Attention

self-attention是指在序列数据中，将当前位置与其他位置之间的关系建模。它通过计算每个位置与其他所有位置之间的相关性得分，从而为每个位置分配一个权重。这使得模型能够根据输入序列的不同部分的重要性，自适应地选择要关注的信息。

target attention 用于将目标与其他相关对象进行比较，并将注意力分配给与目标最相关的对象

### scaled dot-product attention 缩放点积注意力

是transformer用来实现self attention的方式

首先计算查询和键的相似度 $scores = QK^T$

然后缩放，$scaled_scores = \frac{QK^T}{\sqrt{d_k}}$，其中$d_k$是每个查询和键向量的维度，缩放是为了避免点积过大

接下来对查询归一化，得到注意力权重矩阵 $attention_weights = softmax(scaled_scores)$

最后，乘矩阵V，得到加权后的值 $Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

### multi head 多头  （缩写MHA）

多头指的是：对输入执行多次缩放点积注意力，然后拼接（concat）每一个attention，最后过一个o_linear层，就是多头注意力的输出

每个头都代表对一种不同的问题的专注

### layer norm

batch norm : 激活函数会改变数据分布，随着网络加深，这种改变会越来越大，导致训练困难，收敛慢。BN的目的在于进入激活函数之前，变成均值0，方差1的正态分布，避免梯度消失

batch norm 的问题 ： 如果batchsize小，均值方差不足以代表整个数据集；如果太大，会超内存、很难更新（?）

layer norm ：对张良按照某一维度或某几个维度进行均值0、方差1的归一化

还有Group Norm、Instance Norm等等

### feed forward network （缩写是FFN）

标准的transformer的ffn用的激活函数是relu

### positional encoding

目的：让单词具有位置关系，在每个单词的表示上添加了一个与他在句子中位置有关的向量

transformer中，每个词的最终输入是 token embedding + positional encoding，直接相加

### 分词 tokenize

常见的方法：WordPiece、SentencePiece

### 解码策略

llm在next token prediction时，会给出一个概率分布，不同的下一个词对应不同的概率，如何选择？

1. 贪心策略：直接选最大的

2. top-k 采样（也就是调用llm时的参数里的那个） 对概率排名前k的token进行抽样，

3. top-p 采样 ，例如top-p = 0.9，那么选取累计概率超过0.9的集合进行采样（就是从概率最大的开始往里添加，直到这个集合的概率超过0.9）

4. temperature 采样 ， 温度设置的越高，答案越多样化

通常是将 top-k、top-p、Temperature 联合起来使用。使用的先后顺序是 top-k->top-p->Temperature




# 微调

### 基本概念

fine tuning 就是一种迁移学习的方法

PEFT ： 仅训练少量模型参数适应到下游任务，例如更新最后一层的模型参数

模型训傻了的原因：1. 数据偏移，与预训练的数据分布不同； 2. 数据标注有问题 ； 3. 过拟合，微调数据过少； 4. 缺乏多样性，类似3

微调后，通用能力有所下降： 1. 增量学习：领域数据与通用数据交替训练； 2. 强化学习：在通用领域上也设置一定的奖励； ....

### BitFit

只更新bias的参数 ，例如计算qkv的bias、拼接多头注意力之后的o的bias、mlp的bias、ln的bias

### Prefix tuning

固定预训练的LM，为其添加可训练、任务特定的前缀，方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示，并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。

### Prompt Tuning

### Adapter Tuning

在transformer layer的两个残差之前，加上一个adapter layer，这个adapter主要由两个ffn组成，中间细两边粗的结构

### LoRA

低秩分解，对于原本的模型结构旁边加一个旁路，旁路也是两个ffn构成，只不过中间的维度r是矩阵的本征秩，r远小于原本的输入维度d




# 推理

### 推理框架

vllm 适用于大批量prompt的输入，并对推理速度要求高的场景

DeepSpeed 

CTranslate

OpenLLM 

MLC 边缘计算步数LLM

