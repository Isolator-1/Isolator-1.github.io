---
title: 大模型复习
tags: [Others]
date: 2025-08-27 11:40:00
categories: [学习笔记]
excerpt: 为了秋招面试😰
---

```
参考链接 https://github.com/wdndev/llm_interview_note
```

# LLM 概念

### prefix LLM 和 causal LLM

Prefix LLM 中，Encoder和Decoder共享同一个transformer结构，在生成每个词的时候都可以考虑上下文信息，会根据给定的前缀预测下一个词。

Causal LLM 因果语言模型是一种自回归模型，只有decoder，只能根据前文生成后文，即next token prediction，预测下一个词的概率

### 为什么大部分都是decoder only结构

encoder的双向注意力会存在低秩问题，可能削弱模型表达能力，对于生成任务，也不需要前文看后文的能力，没有好处

decoder-only 在没有任何tuning的数据的情况下，zero shot表现最好，encoder-decoder需要在一定量的标注数据上做multitask finetuning才能激发最佳性能

BERT：encoder only

GPT： decoder only

T5： encoder-decoder

早期的transformer一开始是用来做seq2seq2的，所以才长成encoder+decoder的


### 不同类LLM都适合用在什么场合

bert 适用于通用nlp类任务，例如文本分类，语义相似度计算，主要用在不依赖于特定领域知识或语言风格，但用来做对话和问答效果会变差

LLaMa 是一个decoder only的模型，训练语料主要以英文为主，中日韩文效果会变差

ChatGLM 多用于构建聊天机器人，适合多轮对话处理上下文，并且中英训练语料库1：1

### Word2Vec

早期直接用one-hot编码每个token，缺少token之间的自然相似性， 所以google提出了word2vec，但是被google后来提出的bert几乎完全超越

### BERT 

BERT（Bidirectional Encoder Representations from Transformers）是谷歌提出 ， 大幅提升了NLP领域各个任务的精度，产生了质的飞跃

主要贡献：1. 提出了双向transformer框架  2. 使用了Mask Language Model 和 Next Sentence Prediction 的多任务训练目标

Mask LM ：训练时，有15%的单词被替换，其中有80%会替换为[MASK]，10%换成任意token，10%保留原始Token

Next Sentence Prediction ： BERT接收成对的句子作为苏杭如，并且预测第二个句子是否在原始文档中也是后续句子，训练时50%是对应的前后关系，50%是随机断开的两个句子。



